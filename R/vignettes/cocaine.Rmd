---
title: "Cocaine Dependence"
author: "Paul Hendricks"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Cocaine}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

In this vignette, we demonstrate the power of `easyml` using a Cocaine Dependence dataset. 

## Load the data

First we load our libraries and the Cocaine Dependence dataset. 

```{r}
library(easyml)
library(dplyr)
library(ggplot2)

data("cocaine_dependence", package = "easyml")
knitr::kable(head(cocaine_dependence))
```

## Train a random forest model

To run an `easy_random_forest` model, we pass in the following parameters:

* the data set `cocaine_dependence`,
* the name of the dependent variable e.g. `diagnosis`,
* whether to run a `gaussian` or a `binomial` model, 
* which variables to exclude from the analysis, 
* which variables are categorical variables; these variables are not scaled, if `preprocess_scale` is used, 
* the random state, 
* whether to display a progress bar, 
* how many cores to run the analysis on in parallel.

```{r warning=FALSE}
results <- easy_random_forest(cocaine_dependence, "diagnosis",
                              family = "binomial", 
                              exclude_variables = c("subject"),
                              categorical_variables = c("male"),
                              n_samples = 10, n_divisions = 10,
                              n_iterations = 2, progress_bar = FALSE, 
                              random_state = 12345, n_core = 1)

```

## Assess results

Now let's assess the results of the `easy_random_forest` model.

### Estimates of variable importances

TO BE EDITED.

```{r message=FALSE}
results$plot_variable_importances_processed
output <- results$variable_importances_processed
knitr::kable(output, digits  = 2)
```

### Predictions: ROC Curve

We can examine both the in-sample and out-of-sample ROC curve plots for one particular trian-test split determined by the random state and determine the Area Under the Curve (AUC) as a goodness of fit metric. Here, we see that the in-sample AUC is higher than the out-of-sample AUC, but that both metrics indicate the model fits relatively well.

```{r}
results$plot_predictions_train_mean
results$plot_predictions_test_mean
```

### Metrics: AUC

We can examine both the in-sample and out-of-sample AUC metrics for `n_divisions` train-test splits (ususally defaults to 1,000). Again, we see that the in-sample AUC is higher than the out-of-sample AUC, but that both metrics indicate the model fits relatively well.

```{r}
results$plot_metrics_train_mean
results$plot_metrics_test_mean
```

## Discuss

In this vignette we used `easyml` to easily build and evaluate a random forest model TO BE EDITED.
